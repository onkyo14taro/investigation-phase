{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-wichita",
   "metadata": {},
   "source": [
    "# Formatting Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08c4b-2721-4ca5-a14d-3763b4a19a34",
   "metadata": {},
   "source": [
    "In this notebook, we build the same datasets as used in the original LEAF book.  \n",
    "Although the original paper said \"Default train/test splits are always adopted\", there were many datasets that do not have a default split.  \n",
    "Therefore, in this notebook, we set our own split for datasets that do not have a default split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-settle",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-values",
   "metadata": {},
   "source": [
    "## Common functions (run the following block before the other blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io.wavfile\n",
    "import shutil\n",
    "import soundfile\n",
    "\n",
    "\n",
    "def copy_with_format_wave(path_from, path_to, target_rate=16000):\n",
    "    \"\"\"Copy audio data after formatting it (16 kHz, PCM_16, mono).\"\"\"\n",
    "    # Use PCM_16 for memory efficiency to unify bit depth and save data volume.\n",
    "    info = soundfile.info(path_from)\n",
    "    if info.samplerate == target_rate and info.subtype == 'PCM_16' and info.channels == 1:\n",
    "        # Just copy\n",
    "        shutil.copyfile(path_from, path_to)\n",
    "    else:\n",
    "        # Downsampe to 16 kHz and convert to mono (the range becomes [-1.0, 1.0], as a side effect).\n",
    "        wave, sr = librosa.load(path_from, sr=target_rate, mono=True)\n",
    "        # change the range to [-32768, 32767] (PCM_16)\n",
    "        wave = np.clip(wave * 32768, -32768, 32767).astype(np.int16)\n",
    "        scipy.io.wavfile.write(path_to, target_rate, wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-hospital",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-anatomy",
   "metadata": {},
   "source": [
    "## TUT Urban 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-tender",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-signal",
   "metadata": {},
   "source": [
    "Ref: T. Heittola, A. Mesaros, and T. Virtanen, “TUT Urban Acoustic Scenes 2018, Development dataset.” Apr. 2018, doi: 10.5281/zenodo.1228142.\n",
    "\n",
    "Dataset URL: https://zenodo.org/record/1228142#.YJZMsWamO3I\n",
    "\n",
    "* Acoustic scenes\n",
    "* 10 classes\n",
    "* 8,640 samples\n",
    "  * train: 6,122 samples\n",
    "  * eval: 2,518 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-duration",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-contrary",
   "metadata": {},
   "source": [
    "We use the same split as the original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-fellowship",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-ancient",
   "metadata": {},
   "source": [
    "1. Download all data from https://zenodo.org/record/1228142#.YJZMsWamO3I\n",
    "2. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "/foo/bar/TUT_urban_2018/\n",
    "  ├ audio/____.wav\n",
    "  ├ evaluation_setup/____.txt\n",
    "  ├ LICENSE\n",
    "  ├ meta.csv\n",
    "  ├ README.html\n",
    "  └ README.md\n",
    "```\n",
    "\n",
    "3. Format the dataset (48 kHz, 24 bit, 2 ch -> 16 kHz, 16 bit, 1 ch)\n",
    "\n",
    "```\n",
    "../datasets/TUT_urban_2018/\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/TUT_urban_2018')\n",
    "# DIR_FROM = Path('/Users/sky/Documents/__earth/database/TUT_urban_2018')\n",
    "# DIR_TO = Path('../datasets/TUT_urban_2018')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Read meta data (paths and labels)\n",
    "# train_meta = pd.read_csv(DIR_FROM/'evaluation_setup/fold1_train.txt', delimiter='\\t', header=None)\n",
    "# eval_meta = pd.read_csv(DIR_FROM/'evaluation_setup/fold1_evaluate.txt', delimiter='\\t', header=None)\n",
    "# le = LabelEncoder().fit(train_meta['label'].values)\n",
    "\n",
    "# for phase, meta in (('train', train_meta), ('eval', eval_meta)):\n",
    "#     # Format meta data.\n",
    "#     meta_2 = meta.copy()\n",
    "#     meta_2.iloc[:, 0] = meta.iloc[:, 0].str.split('/').map(lambda x: x[1])  # 'audio/xxxx.wav' -> 'xxxx.wav'\n",
    "#     meta_2.columns = ['audio_filename', 'label']  # Header\n",
    "#     meta_2['label_id'] = le.transform(meta_2['label'].values)\n",
    "#     meta_2.to_csv(DIR_TO/f'{phase}.csv', index=False)\n",
    "#     # Format audio data.\n",
    "#     for i, audio_path in enumerate(meta_2.audio_filename):\n",
    "#         print(f'\\r{phase:5} {i:08d} {audio_path}', end='')\n",
    "#         copy_with_format_wave(DIR_FROM/'audio'/audio_path, DIR_TO/phase/audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-immigration",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-pride",
   "metadata": {},
   "source": [
    "## DCASE 2018 Task 3 Bird audio detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-warrior",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-settle",
   "metadata": {},
   "source": [
    "Paper: D. Stowell, M. D. Wood, H. Pamuła, Y. Stylianou, and H. Glotin, “Automatic acoustic detection of birds through deep learning: The first Bird Audio Detection challenge,” Methods Ecol. Evol., vol. 10, no. 3, pp. 368–380, Nov. 2018.\n",
    "\n",
    "Datset URL: http://dcase.community/challenge2018/task-bird-audio-detection\n",
    "\n",
    "* Bird audio detection\n",
    "* 2 classes\n",
    "* 48,310 samples\n",
    "  * train: 35,690 samples\n",
    "    * freefield1010: 7,690 samples\n",
    "    * warblrb10k: 8,000 samples\n",
    "    * BirdVox-DCASE-20k: 20,000 samples\n",
    "  * eval: 12,620 samples (labels not disclosed)\n",
    "    * warblrb10k: 2,000 samples\n",
    "    * Chernobyl: 6,620 samples\n",
    "    * PolandNFC: 4,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-lancaster",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-organizer",
   "metadata": {},
   "source": [
    "DCASE 2018 was a competition and the labels of the evaluation sets are not published.\n",
    "On the other hand, as for warblrb10k, it is used both in training and in evaluation.\n",
    "Therefore, we use warblrb10k as the evaluation set.\n",
    "In summary, the split is as follows:\n",
    "\n",
    "* train: 27,690 samples\n",
    "  * freefield1010: 7,690 samples\n",
    "  * BirdVox-DCASE-20k: 20,000 samples\n",
    "* eval: 8,000 samples\n",
    "  * warblrb10k: 8,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-boxing",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-montreal",
   "metadata": {},
   "source": [
    "1. Download all data from http://dcase.community/challenge2018/task-bird-audio-detection\n",
    "2. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "/foo/bar/DCASE2018_task3_bird_audio/\n",
    "  └ train/\n",
    "    ├ BirdVox-DCASE-20k/____.wav\n",
    "    ├ ff1010bird/____.wav\n",
    "    ├ warblrb10k/____.wav\n",
    "    ├ BirdVoxDCASE20k_csvpublic.csv\n",
    "    ├ ff1010bird_metadata_2018.csv\n",
    "    └ warblrb10k_public_metadata_2018.csv\n",
    "```\n",
    "\n",
    "3. Format the dataset (44.1 kHz, 16 bit, 1 ch -> 16 kHz, 16 bit, 1 ch)\n",
    "\n",
    "```\n",
    "../datasets/DCASE2018_task3_bird_audio/\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/DCASE2018_task3_bird_audio/train')\n",
    "# DIR_FROM = Path('/Users/sky/Documents/__earth/database/DCASE2018_task3_bird_audio/train')\n",
    "# DIR_TO = Path('../datasets/DCASE2018_task3_bird_audio')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Read meta data (paths and labels)\n",
    "# train_meta = pd.concat([\n",
    "#     pd.read_csv(DIR_FROM/'ff1010bird_metadata_2018.csv'),\n",
    "#     pd.read_csv(DIR_FROM/'BirdVoxDCASE20k_csvpublic.csv')\n",
    "# ], axis=0)\n",
    "# eval_meta = pd.read_csv(DIR_FROM/'warblrb10k_public_metadata_2018.csv')\n",
    "\n",
    "# for phase, meta in (('train', train_meta), ('eval', eval_meta)):\n",
    "#     # Format meta data.\n",
    "#     meta_2 = meta.copy()\n",
    "#     meta_2['audio_filename'] = meta['datasetid'].str.cat(meta['itemid'].astype(str), sep='__') + '.wav'\n",
    "#     meta_2['label'] = meta['hasbird']\n",
    "#     meta_2['label_id'] = meta['hasbird']\n",
    "#     meta_2 = meta_2[['audio_filename', 'label', 'label_id']]\n",
    "#     meta_2.to_csv(DIR_TO/f'{phase}.csv', index=False)\n",
    "#     # Format audio data.\n",
    "#     for i, audio_path in enumerate(meta_2.audio_filename):\n",
    "#         print(f'\\r{phase:5} {i:08d} {audio_path}', end='')\n",
    "#         dir_, file_ = audio_path.split('__')\n",
    "#         copy_with_format_wave(DIR_FROM/dir_/file_, DIR_TO/phase/audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-accident",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-aggregate",
   "metadata": {},
   "source": [
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-april",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-oasis",
   "metadata": {},
   "source": [
    "Paper: H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma, “CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset,” IEEE Trans Affect Comput, vol. 5, no. 4, pp. 377–390, Oct. 2014.\n",
    "\n",
    "Datset URL: https://github.com/CheyneyComputerScience/CREMA-D\n",
    "\n",
    "* Emotion recognition\n",
    "* 6 classes\n",
    "* 7,442 samples (no split information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-sunday",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-parking",
   "metadata": {},
   "source": [
    "The original CREMA-D is not set to split. On the other hand, the package `tensorflow_datasets` provided by TensorFlow sets up CREMA-D with split (\"train\", \"validation\", \"test\").\n",
    "If you run the following code in Google colaboratory, you can get speaker information for each partition (it can also be run locally).\n",
    "\n",
    "```python\n",
    "!pip install -q tensorflow-datasets tensorflow pydub\n",
    "```\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from google.colab import files\n",
    "\n",
    "for phase in ('train', 'validation', 'test'):\n",
    "    ds = tfds.load('crema_d', split=phase)\n",
    "    speakers = sorted(set(str(int(ex['speaker_id'].numpy())) for ex in ds.take(-1)))\n",
    "    with open(f'{phase}_speakers.txt', 'w') as f:\n",
    "        f.write('\\n'.join(speakers))\n",
    "    files.download(f'{phase}_speakers.txt')\n",
    "```\n",
    "\n",
    "This time, we set up the split based on the \"train_speakers.csv\", \"validation_speakers.csv\", and \"test_ speakers.csv\" generated by the above code.\n",
    "Among these, we assign \"train_speakers.csv\" for training, and \"validation_speakers.csv\" and \"test_speakers.csv\" for evaluation. \n",
    "In summary, the split is as follows:\n",
    "\n",
    "* train: 5,146 samples (63 speakers)\n",
    "* eval: 2,296 samples\n",
    "  * validation: 738 samples (9 speakers)\n",
    "  * test: 1,558 samples (19 speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-grocery",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-ozone",
   "metadata": {},
   "source": [
    "1. Download all data except for binary data from https://github.com/CheyneyComputerScience/CREMA-D\n",
    "2. Download the binary data using git-lfs.\n",
    "3. Get split information (\"train_speakers.csv\", \"validation_speakers.csv\", and \"test_ speakers.csv\") by using tensorflow_datasets (see section Split).\n",
    "4. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "/foo/bar/CREMA-D/\n",
    "  ├ train_speakers.csv       <- generated by using tensorflow_datasets (see section Split)\n",
    "  ├ validation_speakers.csv  <- generated by using tensorflow_datasets (see section Split)\n",
    "  ├ test_speakers.csv        <- generated by using tensorflow_datasets (see section Split)\n",
    "  ├ AudioMP3/____.mp3\n",
    "  ├ AudioWAV/____.wav\n",
    "  ├ docs/\n",
    "  ├ finishedEmoResponses.csv\n",
    "  ├ finishedResponses.csv\n",
    "  ├ finishedResponsesWithRepeatWithPractice.csv\n",
    "  ├ LICENSE.txt\n",
    "  ├ processedResults/\n",
    "  ├ processFinishedResponses.R\n",
    "  ├ README.md\n",
    "  ├ readTabulatedVotes.R\n",
    "  ├ SentenceFilenames.csv\n",
    "  ├ summarizeVotes.r\n",
    "  ├ tabulateVotesV2.r\n",
    "  ├ VideoDemographics.csv\n",
    "  └ VideoFlash/____.flv\n",
    "```\n",
    "\n",
    "5. Format the dataset (16 kHz, 16 bit, 1 ch -> 16 kHz, 16 bit, 1 ch; no change)\n",
    "\n",
    "```\n",
    "../datasets/CREMA-D/\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/VoxCeleb1')\n",
    "# DIR_FROM = Path('/Users/sky/Documents/__earth/database/CREMA-D/')\n",
    "# DIR_TO = Path('../datasets/CREMA-D')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Format meta data (paths and labels)\n",
    "# with open(DIR_FROM/'train_speakers.txt', 'r') as f:\n",
    "#     train_speakers = f.read().split('\\n')\n",
    "# with open(DIR_FROM/'validation_speakers.txt', 'r') as f:\n",
    "#     eval_speakers = f.read().split('\\n')\n",
    "# with open(DIR_FROM/'test_speakers.txt', 'r') as f:\n",
    "#     eval_speakers += f.read().split('\\n')\n",
    "# train_meta = pd.DataFrame([\n",
    "#     {\n",
    "#         'orig_path': path,\n",
    "#         'audio_filename': path.name,\n",
    "#         'label': path.name.split('_')[2]\n",
    "#     }\n",
    "#     for train_speaker in train_speakers\n",
    "#     for path in (DIR_FROM/'AudioWAV').rglob(f'{train_speaker}*.wav')\n",
    "# ])\n",
    "# eval_meta = pd.DataFrame([\n",
    "#     {\n",
    "#         'orig_path': path,\n",
    "#         'audio_filename': path.name,\n",
    "#         'label': path.name.split('_')[2]\n",
    "#     }\n",
    "#     for eval_speaker in eval_speakers\n",
    "#     for path in (DIR_FROM/'AudioWAV').rglob(f'{eval_speaker}*.wav')\n",
    "# ])\n",
    "# le = LabelEncoder().fit(train_meta['label'].values)\n",
    "# train_meta['label_id'] = le.transform(train_meta['label'].values)\n",
    "# eval_meta['label_id'] = le.transform(eval_meta['label'].values)\n",
    "\n",
    "# for phase, meta in (('train', train_meta), ('eval', eval_meta)):\n",
    "#     # Format meta data.\n",
    "#     meta_2 = meta\n",
    "#     meta_2.drop('orig_path', axis=1).to_csv(DIR_TO/f'{phase}.csv', index=False)\n",
    "#     # Format audio data.\n",
    "#     for i, (orig_path, audio_path) in enumerate(zip(meta_2.orig_path, meta_2.audio_filename)):\n",
    "#         print(f'\\r{phase:5} {i:08d} {audio_path}', end='')\n",
    "#         copy_with_format_wave(orig_path, DIR_TO/phase/audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-civilization",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-antarctica",
   "metadata": {},
   "source": [
    "## VoxCeleb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-device",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-oxide",
   "metadata": {},
   "source": [
    "A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker identification dataset,” arXiv preprint arXiv:1706.08612, 2017.\n",
    "\n",
    "Datset URL: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html\n",
    "\n",
    "* Speaker ID\n",
    "* 153,516 samples (1,251 speakers)\n",
    "  * train: 148,642 samples (1,211 speakers)\n",
    "  * test: 4,874 samples (40 speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-mount",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-links",
   "metadata": {},
   "source": [
    "Use both the original train and test set.\n",
    "For each speaker, split the dataset so that 90% is for training and 10% is for evaluation.\n",
    "To be more precise, at least for each speaker, we tried to have at least 10% of the eval set, so that overall the percentage of evaluation set is more than 10%.\n",
    "In order to increase the difficulty of the task, the YouTube video IDs of the train and eval sets are not duplicated.\n",
    "As a result of executing the code block below, the dataset was divided into the following number of samples:\n",
    "\n",
    "* train: 128,086 samples (1,251 speakers; 83.4%)\n",
    "* eval: 25,430 samples (1,251 speakers; 16.6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-drawing",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-camera",
   "metadata": {},
   "source": [
    "1. Download all data from https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html (form application required)\n",
    "2. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "foo/bar/VoxCeleb1/\n",
    "  ├ train/\n",
    "  │   ├ txt/id____/____/____.txt\n",
    "  │   └ wav/id____/____/____.wav\n",
    "  ├ test/\n",
    "  │   ├ txt/id____/____/____.txt\n",
    "  │   └ wav/id____/____/____.wav\n",
    "  └ vox1_meta.csv\n",
    "```\n",
    "\n",
    "3. Format the dataset (16 kHz, 16 bit, 1 ch -> 16 kHz, 16 bit, 1 ch; no change)\n",
    "\n",
    "```\n",
    "../datasets/VoxCeleb1/\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/VoxCeleb1')\n",
    "# DIR_FROM = Path('/Volumes/ARCHIVE_SSD/Database/_zip/VoxCeleb1')\n",
    "# DIR_TO = Path('../datasets/VoxCeleb1')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Preprocessing\n",
    "# ################################################################################\n",
    "\n",
    "# # Format meta data (audio_filename=speakerID__videoID__{number}.wav, label=speakerID)\n",
    "# master_df = pd.DataFrame([\n",
    "#     {\n",
    "#         'orig_path': path,\n",
    "#         'audio_filename': '__'.join(path.parts[-3:]),\n",
    "#         'video_id': path.parts[-2],\n",
    "#         'label': path.parts[-3]\n",
    "#     }\n",
    "#     for path in DIR_FROM.rglob('*.wav')\n",
    "#     if not path.name.startswith('.')\n",
    "# ])\n",
    "# master_df['label_id'] = LabelEncoder().fit_transform(master_df['label'].values)\n",
    "# train_df = []\n",
    "# eval_df = []\n",
    "\n",
    "\n",
    "# # Process for each label\n",
    "# rng = np.random.RandomState(0)  # Fix random seed.\n",
    "# for label, label_df in master_df.groupby('label'):\n",
    "#     # Split videoIDs so that they do not overlap.\n",
    "#     # The videoIDs are randomized.\n",
    "#     # Train=90%, Eval=10%\n",
    "#     video_ids = list(label_df['video_id'].unique())\n",
    "#     rng.shuffle(video_ids)\n",
    "#     video_ids_count = label_df.groupby('video_id').count().loc[video_ids]\n",
    "#     video_ids_cumsum_count = video_ids_count.cumsum()\n",
    "#     video_ids_relative_cumsum_count = video_ids_cumsum_count / len(label_df)\n",
    "#     train_video_ids \\\n",
    "#         = video_ids_relative_cumsum_count.loc[video_ids_relative_cumsum_count.iloc[:, 0] < 0.9].index\n",
    "#     train_video_ids = sorted(set(train_video_ids))\n",
    "#     eval_video_ids = sorted(set(video_ids).difference(train_video_ids))\n",
    "#     train_df.append(label_df.query('video_id in @train_video_ids'))\n",
    "#     eval_df.append(label_df.query('video_id in @eval_video_ids'))\n",
    "# #     print(f'{label} train={len(train_df[-1]):4} files, eval={len(eval_df[-1]):4} files')\n",
    "# train_df = pd.concat(train_df, axis=0)\n",
    "# eval_df = pd.concat(eval_df, axis=0)\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for phase, df in (('train', train_df), ('eval', eval_df)):\n",
    "#     df.drop('orig_path', axis=1).to_csv(DIR_TO/f'{phase}.csv', index=False)\n",
    "#     # Format audio data.\n",
    "#     for i, (orig_path, audio_filename) in enumerate(zip(df.orig_path, df.audio_filename)):\n",
    "#         print(f'\\r{phase:5} {i:08d} {audio_filename}', end='')\n",
    "#         copy_with_format_wave(orig_path, DIR_TO/phase/audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-receipt",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-orchestra",
   "metadata": {},
   "source": [
    "## NSynth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-counter",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-merchandise",
   "metadata": {},
   "source": [
    "Paper: J. Engel et al., “Neural audio synthesis of musical notes with wavenet autoencoders,” in International Conference on Machine Learning, 2017, pp. 1068–1077.\n",
    "\n",
    "Datset URL: https://magenta.tensorflow.org/datasets/nsynth\n",
    "\n",
    "* Speaker ID\n",
    "* 305,979 samples (11 instruments, 112 pitchs)\n",
    "  * train: 289,205 samples (11 instruments, 112 pitchs)\n",
    "  * valid: 12,678 samples (10 instruments, 112 pitchs)\n",
    "  * test: 4,096 samples (10 instruments, 106 pitchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-hollywood",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-mailman",
   "metadata": {},
   "source": [
    "The original training set is used for training, and the original validation and test set are used for evaluation.\n",
    "In summary, the split is as follows:\n",
    "\n",
    "* train: 289,205 samples (11 instruments, 112 pitchs)\n",
    "* eval: 16,774 samples (10 instruments, 112 pitchs)\n",
    "  * valid: 12,678 samples (10 instruments, 112 pitchs)\n",
    "  * test: 4,096 samples (10 instruments, 106 pitchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-renewal",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-czech",
   "metadata": {},
   "source": [
    "1. Download all data from https://magenta.tensorflow.org/datasets/nsynth\n",
    "2. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "/foo/bar/NSynth/\n",
    "  ├ nsynth-train/\n",
    "  │   ├ audio/____.wav\n",
    "  │   └ examples.json\n",
    "  ├ nsynth-valid/\n",
    "  │   ├ audio/____.wav\n",
    "  │   └ examples.json\n",
    "  └ nsynth-test/\n",
    "      ├ audio/____.wav\n",
    "      └ examples.json\n",
    "```\n",
    "\n",
    "3. Format the dataset (16 kHz, 16 bit, 1 ch -> 16 kHz, 16 bit, 1 ch; no change)\n",
    "\n",
    "```\n",
    "../datasets/NSynth/\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/NSynth')\n",
    "# DIR_FROM = Path('/Users/sky/Documents/__earth/database/NSynth')\n",
    "# DIR_TO = Path('../datasets/NSynth')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Format meta data\n",
    "# train_meta = pd.read_json(DIR_FROM/'nsynth-train/examples.json').T\n",
    "# le_pitch = LabelEncoder().fit(train_meta['pitch'].values)\n",
    "# train_meta['audio_filename'] = train_meta['note_str'] + '.wav'\n",
    "# train_meta['orig_path'] = 'nsynth-train/audio/' + train_meta['audio_filename']\n",
    "# train_meta['pitch_id'] = le_pitch.transform(train_meta['pitch'].values)\n",
    "# train_meta = train_meta[['orig_path', 'audio_filename', 'instrument_family_str', 'instrument_family', 'pitch', 'pitch_id']]\n",
    "# train_meta.columns = ['orig_path', 'audio_filename', 'inst', 'inst_id', 'pitch', 'pitch_id']\n",
    "\n",
    "# valid_meta = pd.read_json(DIR_FROM/'nsynth-valid/examples.json').T\n",
    "# valid_meta['audio_filename'] = valid_meta['note_str'] + '.wav'\n",
    "# valid_meta['orig_path'] = 'nsynth-valid/audio/' + valid_meta['audio_filename']\n",
    "# valid_meta['pitch_id'] = le_pitch.transform(valid_meta['pitch'].values)\n",
    "# valid_meta = valid_meta[['orig_path', 'audio_filename', 'instrument_family_str', 'instrument_family', 'pitch', 'pitch_id']]\n",
    "# valid_meta.columns = ['orig_path', 'audio_filename', 'inst', 'inst_id', 'pitch', 'pitch_id']\n",
    "\n",
    "# test_meta = pd.read_json(DIR_FROM/'nsynth-test/examples.json').T\n",
    "# test_meta['audio_filename'] = test_meta['note_str'] + '.wav'\n",
    "# test_meta['orig_path'] = 'nsynth-test/audio/' + test_meta['audio_filename']\n",
    "# test_meta['pitch_id'] = le_pitch.transform(test_meta['pitch'].values)\n",
    "# test_meta = test_meta[['orig_path', 'audio_filename', 'instrument_family_str', 'instrument_family', 'pitch', 'pitch_id']]\n",
    "# test_meta.columns = ['orig_path', 'audio_filename', 'inst', 'inst_id', 'pitch', 'pitch_id']\n",
    "\n",
    "# eval_meta = pd.concat([valid_meta, test_meta], axis=0)\n",
    "\n",
    "# for phase, meta in (('train', train_meta), ('eval', eval_meta)):\n",
    "#     meta_2 = meta\n",
    "#     meta_2.drop('orig_path', axis=1).to_csv(DIR_TO/f'{phase}.csv', index=False)\n",
    "#     # Format audio data.\n",
    "#     for i, (orig_path, audio_path) in enumerate(zip(meta_2.orig_path, meta_2.audio_filename)):\n",
    "#         print(f'\\r{phase:5} {i:08d} {audio_path}', end='')\n",
    "#         copy_with_format_wave(DIR_FROM/orig_path, DIR_TO/phase/audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-pasta",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-canon",
   "metadata": {},
   "source": [
    "## SpeechCommands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-finish",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-cabin",
   "metadata": {},
   "source": [
    "Paper: P. Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition,” arXiv [cs.CL], Apr. 09, 2018.\n",
    "\n",
    "Datset URL\n",
    "\n",
    "| Version | URL |\n",
    "| :- | :- |\n",
    "| v0.0.1      | http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz |\n",
    "| v0.0.1_test | http://download.tensorflow.org/data/speech_commands_test_set_v0.01.tar.gz |\n",
    "| v0.0.2      | http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz |\n",
    "| v0.0.2_test | http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz |\n",
    "\n",
    "* Speech commands\n",
    "* 35 words\n",
    "* Multiple subsets\n",
    "  * v0.0.1\n",
    "  * v0.0.1_test\n",
    "  * v0.0.2: 105,829 samples (35 words) + 6samples (6 noise types)\n",
    "    * train: 84,843 samples (35 words)\n",
    "    * validation: 9,981 samples (35 words)\n",
    "    * testing: 11,005 samples (35 words)\n",
    "    * noise: 6 samples (6 noise types)\n",
    "  * v0.0.2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-mainland",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-clearance",
   "metadata": {},
   "source": [
    "The original training set is used for training, and the original validation and test set are used for evaluation.\n",
    "In summary, the split is as follows:\n",
    "\n",
    "* train: 84,843 samples (35 words)\n",
    "* eval: 20,986 samples (35 words)\n",
    "  * validation: 9,981 samples (35 words)\n",
    "  * testing: 11,005 samples (35 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-madison",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-egypt",
   "metadata": {},
   "source": [
    "1. Download all data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
    "2. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "/foo/bar/speech_commands_v0.02/\n",
    "  ├ _background_noise_/____.wav\n",
    "  ├ backward/____.wav\n",
    "  ├ bed/____.wav\n",
    "  ├ bird/____.wav\n",
    "  ├ cat/____.wav\n",
    "  ├ dog/____.wav\n",
    "  ├ down/____.wav\n",
    "  ├ eight/____.wav\n",
    "  ├ five/____.wav\n",
    "  ├ follow/____.wav\n",
    "  ├ forward/____.wav\n",
    "  ├ four/____.wav\n",
    "  ├ go/____.wav\n",
    "  ├ happy/____.wav\n",
    "  ├ house/____.wav\n",
    "  ├ learn/____.wav\n",
    "  ├ left/____.wav\n",
    "  ├ marvin/____.wav\n",
    "  ├ nine/____.wav\n",
    "  ├ no/____.wav\n",
    "  ├ off/____.wav\n",
    "  ├ on/____.wav\n",
    "  ├ one/____.wav\n",
    "  ├ right/____.wav\n",
    "  ├ seven/____.wav\n",
    "  ├ sheila/____.wav\n",
    "  ├ six/____.wav\n",
    "  ├ stop/____.wav\n",
    "  ├ three/____.wav\n",
    "  ├ tree/____.wav\n",
    "  ├ two/____.wav\n",
    "  ├ up/____.wav\n",
    "  ├ visual/____.wav\n",
    "  ├ wow/____.wav\n",
    "  ├ yes/____.wav\n",
    "  ├ zero/____.wav\n",
    "  ├ LICENSE\n",
    "  ├ README.md\n",
    "  ├ testing_list.txt\n",
    "  └ validation_list.txt\n",
    "```\n",
    "\n",
    "3. Format the dataset (16 kHz, 16 bit, 1 ch -> 16 kHz, 16 bit, 1 ch; no change)\n",
    "\n",
    "```\n",
    "../datasets/speech_commands_v0.02/\n",
    "  ├ noise/____.wav\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/speech_commands_v0.02')\n",
    "# DIR_FROM = Path('/Users/sky/Documents/__earth/database/speech_commands_v0.02')\n",
    "# DIR_TO = Path('../datasets/speech_commands_v0.02')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'noise').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Format meta data\n",
    "# word_paths = []\n",
    "# noise_paths = []\n",
    "# for p in DIR_FROM.rglob('*.wav'):\n",
    "#     if p.parts[-2] == '_background_noise_':\n",
    "#         noise_paths.append(p)\n",
    "#     else:\n",
    "#         word_paths.append(p)\n",
    "# with open(DIR_FROM/'validation_list.txt', 'r') as f:\n",
    "#     valid_paths = [DIR_FROM/p for p in f.read().split('\\n') if p != '']\n",
    "# with open(DIR_FROM/'testing_list.txt', 'r') as f:\n",
    "#     test_paths = [DIR_FROM/p for p in f.read().split('\\n') if p != '']\n",
    "\n",
    "# eval_paths = sorted(valid_paths + test_paths)\n",
    "# train_paths = sorted(set(word_paths).difference(set(eval_paths)))\n",
    "\n",
    "# train_meta = pd.DataFrame({'orig_path': train_paths})\n",
    "# train_meta['audio_filename'] = train_meta['orig_path'].apply(lambda x: '__'.join(x.parts[-2:]))\n",
    "# train_meta['label'] = train_meta['orig_path'].apply(lambda x: x.parts[-2])\n",
    "# le = LabelEncoder().fit(train_meta['label'].values)\n",
    "# train_meta['label_id'] = le.transform(train_meta['label'].values)\n",
    "\n",
    "# eval_meta = pd.DataFrame({'orig_path': eval_paths})\n",
    "# eval_meta['audio_filename'] = eval_meta['orig_path'].apply(lambda x: '__'.join(x.parts[-2:]))\n",
    "# eval_meta['label'] = eval_meta['orig_path'].apply(lambda x: x.parts[-2])\n",
    "# eval_meta['label_id'] = le.transform(eval_meta['label'].values)\n",
    "\n",
    "# for phase, meta in (('train', train_meta), ('eval', eval_meta)):\n",
    "#     meta_2 = meta\n",
    "#     meta_2.drop('orig_path', axis=1).to_csv(DIR_TO/f'{phase}.csv', index=False)\n",
    "#     # Format audio data.\n",
    "#     for i, (orig_path, audio_path) in enumerate(zip(meta_2.orig_path, meta_2.audio_filename)):\n",
    "#         print(f'\\r{phase:5} {i:08d} {audio_path}', end='')\n",
    "#         copy_with_format_wave(orig_path, DIR_TO/phase/audio_path)\n",
    "\n",
    "# for noise_path in noise_paths:\n",
    "#     copy_with_format_wave(noise_path, DIR_TO/'noise'/noise_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-classification",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-scotland",
   "metadata": {},
   "source": [
    "## Voxforge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-digest",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-discretion",
   "metadata": {},
   "source": [
    "Paper (using Voxforge): S. Revay and M. Teschke, “Multiclass Language Identification using Deep Learning on Spectral Images of Audio Signals,” arXiv [cs.SD], May 10, 2019.\n",
    "\n",
    "Datset URL: www.voxforge.org/\n",
    "\n",
    "* Language ID\n",
    "* 6 languages\n",
    "* No split information\n",
    "* Living dataset (currently update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-birmingham",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-specification",
   "metadata": {},
   "source": [
    "Voxforge is a living database.\n",
    "Here, the dataset is constructed using a fixed list, created by Tensorflow.\n",
    "This list can be downloaded from https://storage.googleapis.com/tfds-data/downloads/voxforge/voxforge_urls.txt.\n",
    "The following command will allow you to download the compressed files in the list.\n",
    "If you are using MacOS and can use Homebrew, you can use the command `wget` after excuting `brew install wget`.\n",
    "\n",
    "```sh\n",
    "wget -P your_target_directory -i /foo/bar/voxforge_urls.txt -x\n",
    "```\n",
    "\n",
    "**Note: Voxforge is a living database. Even if a fixed list created by Tensorflow is used, the audio in the list might be deleted. This might result in a smaller sample size than expected.**\n",
    "\n",
    "For each language, split the dataset so that 90% is for training and 10% is for evaluation.\n",
    "Also, make sure that there is no speaker leakage between the training set and the evaluation set.\n",
    "To be more precise, at least for each language, we tried to have at least 10% of the evaluation set, so that overall the percentage of evaluation set is more than 10%.\n",
    "\n",
    "As a result of executing the code block below, the dataset was divided into the following number of samples:\n",
    "\n",
    "* train: 148,654 samples (6 languages; 84.3%)\n",
    "* eval: 27,764 samples (6 languages; 15.7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-inspection",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-showcase",
   "metadata": {},
   "source": [
    "1. Download fixed data by a fixed list created by TensorFlow.\n",
    "   1. Download the fixed list from https://storage.googleapis.com/tfds-data/downloads/voxforge/voxforge_urls.txt.\n",
    "   2. Download the zipped files by using the following code.\n",
    "      ```sh\n",
    "      wget -P your_target_directory -i /foo/bar/voxforge_urls.txt -x\n",
    "      ```\n",
    "      If you are using MacOS and can use Homebrew, you can use the command `wget` after excuting `brew install wget`.\n",
    "2. Compose the directory tree as follows:\n",
    "\n",
    "```\n",
    "/foo/bar/Voxforge/www.repository.voxforge1.org/downloads/\n",
    "  ├ de/Trunk/Audio/Main/16kHz_16bit/____.tgz\n",
    "  ├ en/Trunk/Audio/Main/16kHz_16bit/____.tgz\n",
    "  ├ es/Trunk/Audio/Main/16kHz_16bit/____.tgz\n",
    "  ├ fr/Trunk/Audio/Main/16kHz_16bit/____.tgz\n",
    "  ├ it/Trunk/Audio/Main/16kHz_16bit/____.tgz \n",
    "  └ ru/Trunk/Audio/Main/16kHz_16bit/____.tgz\n",
    "```\n",
    "\n",
    "3. Format the dataset (16 kHz, 16 bit, 1 ch -> 16 kHz, 16 bit, 1 ch; no change)\n",
    "\n",
    "```\n",
    "../datasets/Voxforge/\n",
    "  ├ eval/____.wav\n",
    "  ├ eval.csv\n",
    "  ├ train/____.wav\n",
    "  └ train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# import time\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ################################################################################\n",
    "# ### Configuration\n",
    "# ################################################################################\n",
    "# # ROOT_DIR = Path('/foo/var/Voxforge')\n",
    "# DIR_FROM = Path('/Volumes/ARCHIVE_SSD/Database/_zip/Voxforge')\n",
    "# DIR_TO = Path('../datasets/Voxforge')\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Preprocessing\n",
    "# ################################################################################\n",
    "# master_df = pd.DataFrame([\n",
    "#     {\n",
    "#         'tgz_path': p,\n",
    "#         'label': p.parts[-6],  # Language\n",
    "#         'speaker': p.stem.split('-')[0]\n",
    "#     }\n",
    "#     for p in DIR_FROM.rglob('*.tgz')\n",
    "# ])\n",
    "# master_df['label_id'] = LabelEncoder().fit_transform(master_df['label'].values)\n",
    "# train_df = []\n",
    "# eval_df = []\n",
    "\n",
    "# # Process for each label (language)\n",
    "# rng = np.random.RandomState(0)  # Fix random seed.\n",
    "# for label, label_df in master_df.groupby('label'):\n",
    "#     # Split speakers so that they do not overlap.\n",
    "#     # The speakers are randomized.\n",
    "#     # However, \"anonymous\" is always included in training.\n",
    "#     # Train=90%, Eval=10%\n",
    "#     speakers_non_anonymous = list(label_df.query('speaker != \"anonymous\"')['speaker'].unique())\n",
    "#     rng.shuffle(speakers_non_anonymous)\n",
    "#     speakers = ['anonymous'] + speakers_non_anonymous\n",
    "#     speakers_count = label_df.groupby('speaker').count().loc[speakers]\n",
    "#     speakers_cumsum_count = speakers_count.cumsum()\n",
    "#     speakers_relative_cumsum_count = speakers_cumsum_count / len(label_df)\n",
    "#     train_speakers \\\n",
    "#         = speakers_relative_cumsum_count.loc[speakers_relative_cumsum_count.iloc[:, 0] < 0.9].index\n",
    "#     train_speakers = sorted(set(train_speakers))\n",
    "#     eval_speakers = sorted(set(speakers).difference(train_speakers))\n",
    "#     train_df.append(label_df.query('speaker in @train_speakers'))\n",
    "#     eval_df.append(label_df.query('speaker in @eval_speakers'))\n",
    "#     print(f'{label} train={len(train_df[-1]):4} zips, eval={len(eval_df[-1]):4} zips')\n",
    "# train_df = pd.concat(train_df, axis=0)\n",
    "# eval_df = pd.concat(eval_df, axis=0)\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ### Processing\n",
    "# ################################################################################\n",
    "# # Make directories.\n",
    "# (DIR_TO/'train').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'eval').mkdir(parents=True, exist_ok=True)\n",
    "# (DIR_TO/'_temp').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for phase, df in (('train', train_df), ('eval', eval_df)):\n",
    "#     meta = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         tmp_dir = DIR_TO/'_temp'/row.tgz_path.stem\n",
    "#         try:\n",
    "#             with tarfile.open(row.tgz_path, 'r') as f:\n",
    "#                 f.extractall(path=tmp_dir)\n",
    "#         except tarfile.ReadError as e:\n",
    "#             print('/'.join(row.tgz_path.parts[-8:]))\n",
    "#         for orig_path in Path(tmp_dir).rglob('*.wav'):\n",
    "#             audio_filename = f'{row.label}__{row.tgz_path.stem}__{orig_path.name}'\n",
    "#             meta.append({\n",
    "#                 'audio_filename': audio_filename,\n",
    "#                 'label': row.label,\n",
    "#                 'label_id': row.label_id,\n",
    "#                 'speaker': row.speaker if row.speaker != 'anonymous' else None\n",
    "#             })\n",
    "#             copy_with_format_wave(orig_path, DIR_TO/phase/audio_filename)\n",
    "#     pd.DataFrame(meta).to_csv(DIR_TO/f'{phase}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-return",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "*End*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
